{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "with open('annotations/train.json', 'r') as f:\n",
    "  train = json.load(f)\n",
    "train.keys()\n",
    "with open('annotations/validate.json', 'r') as f:\n",
    "  valid = json.load(f)\n",
    "valid.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('train',exist_ok=True)\n",
    "os.makedirs('valid',exist_ok=True)\n",
    "\n",
    "for images in valid['images']:\n",
    "    fname = images['file_name']\n",
    "    type_image = fname.split('_')[1]\n",
    "    shutil.copy(f'{type_image}/{fname}',f'valid/{fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pycocotools.coco as coco\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./'\n",
    "dataType='train2017'\n",
    "annFile='{}annotations/custom_train.json'.format(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO api for instance annotations\n",
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display COCO categories and supercategories\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "\n",
    "nms=[cat['name'] for cat in cats]\n",
    "print('Categories: {}'.format(nms))\n",
    "\n",
    "nms = set([cat['supercategory'] for cat in cats])\n",
    "print('Super-categories: {}'.format(nms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display image\n",
    "catIds = coco.getCatIds(catNms=['human'])\n",
    "imgIds = coco.getImgIds(catIds=catIds )\n",
    "\n",
    "img_id = imgIds[np.random.randint(0,len(imgIds))]\n",
    "print('Image nÂ°{}'.format(img_id))\n",
    "\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "\n",
    "img_name = '%s/%s/%s'%(dataDir, dataType, img['file_name'])\n",
    "print('Image name: {}'.format(img_name))\n",
    "\n",
    "I = io.imread(img_name)\n",
    "plt.figure()\n",
    "plt.imshow(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds)\n",
    "anns = coco.loadAnns(annIds)\n",
    "# load and display instance annotations\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights\n",
    "\n",
    "Load a check-point (urls can be found [here](https://github.com/facebookresearch/detr#model-zoo)), then remove the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "first_class_index = 0\n",
    "num_classes = 1\n",
    "finetuned_classes = ['human']\n",
    "\n",
    "print('First class index: {}'.format(first_class_index))  \n",
    "print('Parameter num_classes: {}'.format(num_classes))\n",
    "print('Fine-tuned classes: {}'.format(finetuned_classes))\n",
    "\n",
    "model = torch.hub.load('facebookresearch/detr',\n",
    "                       'detr_resnet101',\n",
    "                       pretrained=False,\n",
    "                       num_classes=num_classes)\n",
    "\n",
    "checkpoint = torch.load('models/detr_40_epoch.pth',\n",
    "                        map_location='cuda')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'],\n",
    "                      strict=False)\n",
    "model.to('cuda')\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/woctezuma/detr.git\n",
    "\n",
    "%cd detr/\n",
    "\n",
    "!git checkout finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py \\\n",
    "  --dataset_file \".\" \\\n",
    "  --coco_path $dataDir \\\n",
    "  --output_dir \"outputs\" \\\n",
    "  --resume \"detr-r101_no-class-head.pth\" \\\n",
    "  --num_classes $num_classes \\\n",
    "  --epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/train2017/000000310645.jpg'\n",
    "im = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "          \n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).to('cuda')\n",
    "    return b\n",
    "\n",
    "def plot_finetuned_results(pil_img, prob=None, boxes=None, save_dir=False, one_box=True):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if one_box:\n",
    "      if prob is not None and boxes is not None:\n",
    "        highest_prob_idx = torch.argmax(prob,axis=1)\n",
    "        boxes = boxes[highest_prob_idx]\n",
    "        prob = prob[highest_prob_idx]\n",
    "        for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                      fill=False, color=c, linewidth=3))\n",
    "            cl = p.argmax()\n",
    "            text = f'{finetuned_classes[cl]}: {p[cl]:0.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize=15,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    else:\n",
    "      if prob is not None and boxes is not None:\n",
    "        for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                      fill=False, color=c, linewidth=3))\n",
    "            cl = p.argmax()\n",
    "            text = f'{finetuned_classes[cl]}: {p[cl]:0.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize=15,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    if save_dir:\n",
    "      plt.savefig(save_dir)\n",
    "    plt.show()\n",
    "\n",
    "def filter_bboxes_from_outputs(outputs,\n",
    "                               threshold=0.7):\n",
    "  \n",
    "  # keep only predictions with confidence above threshold\n",
    "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "\n",
    "  probas_to_keep = probas[keep]\n",
    "\n",
    "  # convert boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], pil_image.size)\n",
    "  \n",
    "  return probas_to_keep, bboxes_scaled\n",
    "\n",
    "def run_worflow(my_image, my_model, save_dir=False):\n",
    "  # mean-std normalize the input image (batch-size: 1)\n",
    "  img = transform(my_image).unsqueeze(0)\n",
    "  img = img.to('cuda')\n",
    "  # propagate through the model\n",
    "  # DETR-RESNET101 takes 0.12s/120ms\n",
    "  outputs = my_model(img)\n",
    "\n",
    "  for threshold in [0.9, 0.7]:\n",
    "    \n",
    "    probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs,\n",
    "                                                              threshold=threshold)\n",
    "\n",
    "    plot_finetuned_results(my_image,\n",
    "                           probas_to_keep, \n",
    "                           bboxes_scaled,\n",
    "                           save_dir=save_dir)\n",
    "  if len(probas_to_keep) > 1:\n",
    "    return torch.max(probas_to_keep)\n",
    "  if len(probas_to_keep) == 1:\n",
    "    return probas_to_keep\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sample import SAMPLE\n",
    "\n",
    "\n",
    "\n",
    "np.nan_to_num(0)\n",
    "\n",
    "# If the difference between the hottest point and the coolest point is below a threshold value\n",
    "# make the image blank as it is highly likely to be ambient data\n",
    "# For testing purposes\n",
    "# sample = td_to_img(sample,tmin,tmax)\n",
    "\n",
    "# Image processing to resize for model\n",
    "# Colour grayscale image\n",
    "img = cv2.applyColorMap(cv2.bitwise_not(np.array(SAMPLE).astype(np.uint8)), cv2.COLORMAP_JET)\n",
    "# Upscale Image\n",
    "img = cv2.resize(img, (640,480), interpolation = cv2.INTER_CUBIC)\n",
    "pil_image = Image.fromarray(img)\n",
    "run_worflow(pil_image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "outputs = []\n",
    "for fname in glob.glob('data/complete_thermal_image/*.png'):\n",
    "    img = Image.open(fname)\n",
    "    file = fname.split('\\\\')[-1]\n",
    "    file_type = file.split('_')[1]\n",
    "    # Image processing\n",
    "    os.makedirs('output',exist_ok=True)\n",
    "    prediction = run_worflow(img, model, save_dir=f'output\\{file}')\n",
    "    if prediction > 0:\n",
    "        predict = 'human'\n",
    "    else:\n",
    "        predict = 'nuisance'\n",
    "    outputs.append(f'{predict}+{file_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    Truth\n",
    "                Nusiance Human\n",
    "                \n",
    "      Nusiance    556     154\n",
    "\n",
    "      Human        1      1075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('money')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2d971e524afe961ec82e7f30ef125a0e603d4c2953e19d850a6a14cdbd6eab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
