{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microphone Streaming Examples\n",
    "\n",
    "A simple notebook that uses Serial/Pyaudio to get the microphone audio and feeds this audio then to Silero VAD.\n",
    "\n",
    "I created it as an example on how binary data from a stream could be feed into Silero VAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.20.2\n",
    "# !pip install torch==1.9.0\n",
    "# !pip install matplotlib==3.4.2\n",
    "# !pip install torchaudio==0.9.0\n",
    "# !pip install soundfile==0.10.3.post1\n",
    "# !pip install pyaudio==0.2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Backend \"soundfile\" is not one of available backends: [].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f33c57887ece>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_audio_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"soundfile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cawin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchaudio\\backend\\utils.py\u001b[0m in \u001b[0;36mset_audio_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_audio_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         raise RuntimeError(\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;34mf'Backend \"{backend}\" is not one of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             f'available backends: {list_audio_backends()}.')\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Backend \"soundfile\" is not one of available backends: []."
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "torch.set_num_threads(60)\n",
    "import torchaudio\n",
    "from time import time\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "import pyaudio\n",
    "\n",
    "import serial\n",
    "from jupyterplot import ProgressPlot\n",
    "import threading\n",
    "\n",
    "ENABLE_LAPTOP_MIC = True # Uses laptop microphone instead of arduino microphone\n",
    "SAMPLE_250ms_audio = False # Reads the audio as 250ms chunks from the microphone, converts them to a Pytorch Tensor, and gets the probabilities/confidences if the model thinks the frame is voiced.\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "# Provided by Alexander Veysov\n",
    "def int2float(sound):\n",
    "    abs_max = np.abs(sound).max()\n",
    "    sound = sound.astype('float32')\n",
    "    if abs_max > 0:\n",
    "        sound *= 1/abs_max\n",
    "    sound = sound.squeeze()  # depends on the use case\n",
    "    return sound\n",
    "\n",
    "if ENABLE_LAPTOP_MIC:\n",
    "    # Pyaudio setup\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    SAMPLE_RATE = 16000\n",
    "    CHUNK = int(SAMPLE_RATE / 10)\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    num_samples = 1536\n",
    "\n",
    "    if SAMPLE_250ms_audio:\n",
    "        stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=SAMPLE_RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK,\n",
    "                    input_device_index=0)\n",
    "        data = []\n",
    "        voiced_confidences = []\n",
    "\n",
    "        print(\"Started Recording\")\n",
    "        for i in range(0, 100):\n",
    "            \n",
    "            audio_chunk = stream.read(num_samples)\n",
    "            \n",
    "            # in case you want to save the audio later\n",
    "            data.append(audio_chunk)\n",
    "            \n",
    "            audio_int16 = np.frombuffer(audio_chunk, np.int16);\n",
    "\n",
    "            audio_float32 = int2float(audio_int16)\n",
    "            \n",
    "            # get the confidences and add them to the list to plot them later\n",
    "            new_confidence = model(torch.from_numpy(audio_float32), 16000).item()\n",
    "            voiced_confidences.append(new_confidence)\n",
    "            \n",
    "        print(\"Stopped the recording\")\n",
    "\n",
    "        # plot the confidences for the speech\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(voiced_confidences)\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Real Time Visualization\n",
    "        # In contrast to the simeple one, this records the audio until to stop the recording by pressing enter.\n",
    "        \n",
    "        continue_recording = True\n",
    "\n",
    "        def stop():\n",
    "            input(\"Press Enter to stop the recording:\")\n",
    "            global continue_recording\n",
    "            continue_recording = False\n",
    "\n",
    "        def start_recording():\n",
    "            \n",
    "            stream = audio.open(format=FORMAT,\n",
    "                            channels=CHANNELS,\n",
    "                            rate=SAMPLE_RATE,\n",
    "                            input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "            data = []\n",
    "            voiced_confidences = []\n",
    "            \n",
    "            global continue_recording\n",
    "            continue_recording = True\n",
    "            \n",
    "            pp = ProgressPlot(plot_names=[\"Silero VAD\"],line_names=[\"speech probabilities\"], x_label=\"audio chunks\")\n",
    "            \n",
    "            stop_listener = threading.Thread(target=stop)\n",
    "            stop_listener.start()\n",
    "            \n",
    "            while continue_recording:\n",
    "            \n",
    "                # 3072 bytes\n",
    "                audio_chunk = stream.read(num_samples)\n",
    "                # print(\"audio_chunk\",audio_chunk)\n",
    "\n",
    "                # 1536 samples \n",
    "                audio_int16 = np.frombuffer(audio_chunk, np.int16);\n",
    "                # print(\"audio_int16\",audio_int16)\n",
    "\n",
    "                # 1536 samples \n",
    "                audio_float32 = int2float(audio_int16)\n",
    "                # print(\"audio_float32\",audio_float32)\n",
    "            \n",
    "                # get the confidences and add them to the list to plot them later\n",
    "                new_confidence = model(torch.from_numpy(audio_float32), 16000).item()\n",
    "                voiced_confidences.append(new_confidence)\n",
    "            \n",
    "                pp.update(new_confidence)\n",
    "\n",
    "            pp.finalize()\n",
    "        start_recording()\n",
    "\n",
    "else:\n",
    "    ser = serial.Serial('COM4', 115200)\n",
    "    pp = ProgressPlot(plot_names=[\"Silero VAD\"],line_names=[\"speech probabilities\"], x_label=\"audio chunks\")\n",
    "    counter = 0 \n",
    "    start = time()\n",
    "    lst = []\n",
    "    data = []\n",
    "    voiced_confidences = []\n",
    "    while True:\n",
    "        sample = ser.readline()\n",
    "        try:\n",
    "            sample = sample.decode('utf-8') # decode from byte to string\n",
    "            sample = sample.split(',')\n",
    "            arr = np.array(sample)\n",
    "            arr = arr.astype(np.int16)\n",
    "            audio_float32 = int2float(arr)\n",
    "            # get the confidences and add them to the list to plot them later\n",
    "            new_confidence = model(torch.from_numpy(audio_float32), 16000).item()\n",
    "            voiced_confidences.append(new_confidence)\n",
    "            pp.update(new_confidence)\n",
    "        except:\n",
    "            pass\n",
    "    pp.finalize()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78a5ae3d11c85b6c890ed05d5a681e432b1936b8567a47c6f4b053e6e90d1449"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
